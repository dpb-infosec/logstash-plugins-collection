# encoding: utf-8
require "logstash/outputs/base"
require "logstash/namespace"
require "logstash/plugin_mixins/normalize_config_support"
require "thread"
require "find"
require "concurrent"
require "json"
require "fileutils"

module CONN_STATUS
  OPEN = 0
  CLOSED = 1
end

# A small struct-like class for a TCP connection
class SocketLB
  attr_accessor :tcp_socket, :state, :messages_sent

  def initialize(tcp_socket, state)
    @tcp_socket = tcp_socket
    @state = state
    @messages_sent = 0
  end
end

# ----------------------------------------------------------------------------- 
# MultiFileQueue: A disk‐based queue that splits logs across multiple files.
#
# This class maintains an in‐memory buffer for lines to be written to disk.
# When the buffer reaches a configured size or a time interval elapses, the 
# buffer’s contents are flushed to disk (in one joined write). Disk files are 
# rotated when their size reaches @disk_queue_file_max_size. A pointer file (in JSON) 
# maintains the current read/write file indices and read offset.
#
# Thread safety:
# - Two mutexes are used: 
#   • @buffer_queue_mutex for the in‐memory buffer.
#   • @disk_queue_mutex for operations on disk state (writes, file rotation,
#     reading, and disk usage updates).
#
# For Option B, we enforce a consistent lock ordering:
# - Any operation that needs both locks first acquires @buffer_queue_mutex, 
#   then releases it and acquires @disk_queue_mutex.
#
# Periodic flushes (via a TimerTask) and pops both call flush_queue_if_needed 
# (which snapshots the in‑memory buffer under the buffer lock, then writes it to 
# disk under the disk lock), avoiding nested locks.
# ----------------------------------------------------------------------------- 
class MultiFileQueue
  def initialize(queue_dir, pointer_file_path, file_buffer_queue_size, logger, disk_queue_file_max_size, max_disk_queue_size)
    @logger = logger
    @queue_dir = queue_dir
    @pointer_file_path = pointer_file_path
    @file_buffer_queue_size = file_buffer_queue_size
    @disk_queue_file_max_size = disk_queue_file_max_size

    # If 0, then no overall disk limit is imposed. Otherwise, parsed as bytes.
    @max_disk_queue_size_bytes = max_disk_queue_size.to_i > 0 ? max_disk_queue_size.to_i : 0

    # Initialize total disk usage from pre-existing queue files.
    @disk_usage = 0
    base = File.basename(@pointer_file_path, ".offset")
    Dir.glob(File.join(@queue_dir, "#{base}_*.queue")).each do |f|
      @disk_usage += File.size?(f) || 0
    end

    FileUtils.mkdir_p(@queue_dir) unless File.directory?(@queue_dir)

    # In-memory buffer and its mutex.
    @buffer_queue = []
    @buffer_queue_mutex = Mutex.new
    @current_buffer_bytes = 0

    # We'll track the time of the last push, for inactivity-based flushes.
    @last_push_time = Time.now
    # Set how long of no new pushes before we flush forcibly:
    @inactivity_interval = 3  # 3 seconds

    # Disk queue mutex and condition variable.
    @disk_queue_mutex = Mutex.new
    @disk_queue_not_empty = ConditionVariable.new

    # @queue_size is the approximate line count stored on disk.
    @queue_size = 0

    # Pointers for file names and reading.
    @current_read_file_index = 0
    @read_offset = 0
    @current_write_file_index = 0
    @current_read_file = nil

    load_or_init_pointer_file
    open_current_write_file

    # Determine the initial disk line count.
    @queue_size = approximate_line_count
    @last_flush_time = Time.now

    # A periodic task to handle:
    #   1) Flushing if older than 1 second (your existing logic)
    #   2) Flushing if inactive for 3 seconds
    @flush_task = Concurrent::TimerTask.new(execution_interval: 1, run_now: false) do
      flush_queue_if_needed  # your existing 1-second check
    end
    @flush_task.execute
  end

  # ------------------------------------------------------------------------- 
  # push(item)
  #
  # Push an item (log line) into the in-memory buffer. 
  #  - If adding this item would exceed @file_buffer_queue_size, forcibly flush first.
  #  - Otherwise, run the "time-based" flush check.
  #  - Update @last_push_time so we know we have fresh activity.
  # -------------------------------------------------------------------------
  def push(item)
    @buffer_queue_mutex.synchronize do
      return if item.nil? || item.strip.empty?
      
      item = item.chomp("\n") + "\n"
      item_size = item.bytesize

      # Check total size in memory
      if @current_buffer_bytes + item_size >= @file_buffer_queue_size
        flush_queue_force_no_lock # forcibly flush, ignoring time checks
      end

      @buffer_queue << item
      @current_buffer_bytes += item_size
      @last_push_time = Time.now
    end
  end

  # -------------------------------------------------------------------------
  # pop
  #
  # Pops one line from the disk-based portion of the queue.
  # Before reading from disk, do a time-based flush to be sure in-memory data 
  # is on disk (if it meets the threshold).
  # -------------------------------------------------------------------------
  def pop
    @disk_queue_mutex.synchronize do
      while @queue_size == 0
        @disk_queue_not_empty.wait(@disk_queue_mutex)
      end
      read_next_line
    end
  end

  # -------------------------------------------------------------------------
  # size
  #
  # Returns the total number of lines currently stored (in memory and on disk).
  # A flush is performed before reading sizes for consistency.
  # -------------------------------------------------------------------------
  def size
    # read the buffer size with the buffer lock only
    buf_size = 0
    @buffer_queue_mutex.synchronize do
      buf_size = @buffer_queue.size
    end
  
    # then read the disk size with the disk lock only
    disk_size = 0
    @disk_queue_mutex.synchronize do
      disk_size = @queue_size
    end
  
    buf_size + disk_size
  end

  # -------------------------------------------------------------------------
  # flush_queue_if_needed
  #
  # Existing logic: 
  # If the buffer is nonempty and the time since the last flush is > 1 second, 
  # flush the buffer to disk. 
  #
  # This is called outside the buffer lock, so we reacquire it inside.
  # -------------------------------------------------------------------------
  def flush_queue_if_needed
    data_str = nil
    count = 0

    @buffer_queue_mutex.synchronize do
      if @buffer_queue.any? && (Time.now - @last_flush_time > 1)
        data_str = @buffer_queue.join
        count = @buffer_queue.size
        @buffer_queue.clear
        @current_buffer_bytes = 0
        @last_flush_time = Time.now
      end
    end

    if data_str
      @disk_queue_mutex.synchronize do
        write_data_to_current_file(data_str)
        @queue_size += count
        @disk_queue_not_empty.signal
      end
    end
  end

  # -------------------------------------------------------------------------
  # flush_queue_if_needed_no_lock
  #
  # Same logic as flush_queue_if_needed, but we assume the caller already
  # holds @buffer_queue_mutex. Useful inside push().
  # -------------------------------------------------------------------------
  def flush_queue_if_needed_no_lock
    if @buffer_queue.any? && (Time.now - @last_flush_time > 1)
      data_str = @buffer_queue.join
      count = @buffer_queue.size
      @buffer_queue.clear
      @current_buffer_bytes = 0
      @last_flush_time = Time.now

      @disk_queue_mutex.synchronize do
        write_data_to_current_file(data_str)
        @queue_size += count
        @disk_queue_not_empty.signal
      end
    end
  end

  # -------------------------------------------------------------------------
  # flush_queue_force
  #
  # Public method that forcibly flushes the buffer to disk, ignoring time checks.
  # (It acquires @buffer_queue_mutex by itself.)
  # -------------------------------------------------------------------------
  def flush_queue_force
    @buffer_queue_mutex.synchronize do
      flush_queue_force_no_lock
    end
  end

  # -------------------------------------------------------------------------
  # flush_queue_force_no_lock
  #
  # Forcible flush if the buffer is non-empty, 
  # to be called *while already holding @buffer_queue_mutex*.
  # -------------------------------------------------------------------------
  def flush_queue_force_no_lock
    return if @buffer_queue.empty?

    data_str = @buffer_queue.join
    count    = @buffer_queue.size
    @buffer_queue.clear
    @current_buffer_bytes = 0
    @last_flush_time = Time.now

    @disk_queue_mutex.synchronize do
      write_data_to_current_file(data_str)
      @queue_size += count
      @disk_queue_not_empty.signal
    end
  end

  # -------------------------------------------------------------------------
  # stop
  #
  # Signals the queue to stop
  # -------------------------------------------------------------------------
  def stop
    @disk_queue_mutex.synchronize do
      # wake up anyone waiting on @disk_queue_not_empty
      @disk_queue_not_empty.broadcast
    end
  end

  private

  # -------------------------------------------------------------------------
  # write_data_to_current_file(data_str)
  #
  # Writes the given data to the current file, rotating if the addition
  # would exceed @disk_queue_file_max_size. Also updates the tracked disk usage and
  # checks against @max_disk_queue_size_bytes.
  # (Caller must hold @disk_queue_mutex.)
  # -------------------------------------------------------------------------
  def write_data_to_current_file(data_str)
    rotate_if_needed(data_str.bytesize)
    @current_write_file.write(data_str)
    @current_write_file.flush

    @disk_usage += data_str.bytesize
    check_disk_usage_limit
  end

  # -------------------------------------------------------------------------
  # rotate_if_needed(new_data_bytes)
  #
  # Rotates to a new file if adding new_data_bytes to the current file's size 
  # would exceed @disk_queue_file_max_size.
  # -------------------------------------------------------------------------
  def rotate_if_needed(new_data_bytes)
    current_size = @current_write_file.size
    if (current_size + new_data_bytes) > @disk_queue_file_max_size
      @current_write_file.close
      @current_write_file_index += 1
      update_pointer_file(@current_read_file_index, @read_offset, @current_write_file_index)
      open_current_write_file
    end
  end

  # -------------------------------------------------------------------------
  # read_next_line
  #
  # Reads one line from disk from the file indicated by @current_read_file_index.
  # Updates the file offset and reduces @queue_size accordingly.
  # (Caller must hold @disk_queue_mutex.)
  # -------------------------------------------------------------------------
  def read_next_line
    line = nil
    path = file_path_for_index(@current_read_file_index)
    if @current_read_file.nil? || @current_read_file.closed?
      unless File.exist?(path)
        try_advance_to_next_file
        return nil
      end
      @current_read_file = File.open(path, "rb")
      @current_read_file.seek(@read_offset, IO::SEEK_SET)
    end

    begin
      line = @current_read_file.gets
      if !line.nil? && !line.strip.empty?
        @read_offset += line.bytesize
        @queue_size -= 1
        update_pointer_file(@current_read_file_index, @read_offset, @current_write_file_index)
      else
        # End-of-file reached; advance to next file.
        @current_read_file.close rescue nil
        try_advance_to_next_file
      end
    rescue EOFError
      line = nil
    rescue IOError => e
      @logger.error("IOError on pop: #{e.message}")
      line = nil
    end
    line
  end

  # -------------------------------------------------------------------------
  # try_advance_to_next_file
  #
  # Closes the current read file (if open), deletes the finished file from disk,
  # updates disk usage, advances the read pointer and resets the read offset.
  # (Caller must hold @disk_queue_mutex.)
  # -------------------------------------------------------------------------
  def try_advance_to_next_file
    if @current_read_file && !@current_read_file.closed?
      @current_read_file.close rescue nil
    end

    old_path = file_path_for_index(@current_read_file_index)
    if File.exist?(old_path)
      file_size = File.size(old_path) rescue 0
      File.delete(old_path) rescue nil
      @disk_usage -= file_size
    end
    @current_read_file_index += 1
    @read_offset = 0
    update_pointer_file(@current_read_file_index, @read_offset, @current_write_file_index)

    new_path = file_path_for_index(@current_read_file_index)
    if File.exist?(new_path)
      @current_read_file = File.open(new_path, "rb")
    else
      @current_read_file = nil
    end
  end

  # -------------------------------------------------------------------------
  # check_disk_usage_limit
  #
  # Removes the oldest files until the total disk usage is below the specified
  # max_disk_queue_size_bytes. (Caller must hold @disk_queue_mutex.)
  # -------------------------------------------------------------------------
  def check_disk_usage_limit
    while @max_disk_queue_size_bytes > 0 &&
          @disk_usage > @max_disk_queue_size_bytes &&
          @current_read_file_index < @current_write_file_index
      remove_oldest_file
    end
  end

  # -------------------------------------------------------------------------
  # remove_oldest_file
  #
  # Deletes the file at @current_read_file_index from disk, updating disk usage,
  # then advances the read pointer.
  # (Caller must hold @disk_queue_mutex.)
  # -------------------------------------------------------------------------
  def remove_oldest_file
    old_path = file_path_for_index(@current_read_file_index)
    if File.exist?(old_path)
      file_size = File.size(old_path) rescue 0
      File.delete(old_path) rescue nil
      @disk_usage -= file_size
    end
    @current_read_file_index += 1
    @read_offset = 0
    update_pointer_file(@current_read_file_index, @read_offset, @current_write_file_index)
  end

  # -------------------------------------------------------------------------
  # approximate_line_count
  #
  # Estimates the total number of lines stored on disk between the current
  # read and write pointers. (Preferably not called concurrently.)
  # -------------------------------------------------------------------------
  def approximate_line_count
    count = 0
    @current_read_file_index.upto(@current_write_file_index) do |idx|
      path = file_path_for_index(idx)
      next unless File.exist?(path)
      File.open(path, "rb") do |f|
        f.seek(@read_offset, IO::SEEK_SET) if idx == @current_read_file_index
        f.each_line { count += 1 }
      end
    end
    count
  rescue => e
    @logger.warn("Error counting lines in queue files: #{e.message}")
    0
  end

  # -------------------------------------------------------------------------
  # load_or_init_pointer_file
  #
  # Loads pointer data from the JSON file if it exists, otherwise creates it.
  # -------------------------------------------------------------------------
  def load_or_init_pointer_file
    if File.exist?(@pointer_file_path) && !File.zero?(@pointer_file_path)
      data = JSON.parse(File.read(@pointer_file_path))
      @current_read_file_index  = data["read_file_index"].to_i
      @read_offset              = data["read_offset"].to_i
      @current_write_file_index = data["write_file_index"].to_i
    else
      update_pointer_file(@current_read_file_index, @read_offset, @current_write_file_index)
    end
  end

  # -------------------------------------------------------------------------
  # update_pointer_file
  #
  # Writes the current pointer values (read file index, read offset, write file index)
  # into the pointer file in JSON format.
  # -------------------------------------------------------------------------
  def update_pointer_file(read_idx, offset, write_idx)
    data = {
      "read_file_index"  => read_idx,
      "read_offset"      => offset,
      "write_file_index" => write_idx
    }
    File.open(@pointer_file_path, "w") { |f| f.write(data.to_json) }
  rescue => e
    @logger.error("Failed to update pointer file: #{e.message}")
  end

  # -------------------------------------------------------------------------
  # open_current_write_file
  #
  # Opens the current write file in append mode.
  # -------------------------------------------------------------------------
  def open_current_write_file
    path = file_path_for_index(@current_write_file_index)
    @current_write_file = File.open(path, "a+b")
    @current_write_file.sync = true
    @current_write_file.seek(0, IO::SEEK_END)
  end

  # -------------------------------------------------------------------------
  # file_path_for_index
  #
  # Constructs a full file path for the given file index based on the pointer file.
  # -------------------------------------------------------------------------
  def file_path_for_index(idx)
    base = File.basename(@pointer_file_path, ".offset")
    sprintf("%s/%s_%06d.queue", @queue_dir, base, idx)
  end
end

# ----------------------------------------------------------------------------- 
# CustomQueue: A hybrid memory/disk queue.
#
# This class holds a memory buffer (limited by a byte-size) for events. 
# When events exceed the in-memory limit, they are pushed to the MultiFileQueue (disk).
# Thread safety is ensured via its own @queue_mutex. 
# The CustomQueue delegates disk operations to MultiFileQueue.
# ----------------------------------------------------------------------------- 
class CustomQueue
  def initialize(mem_max_bytes, file_queue_dir, pointer_file_path, file_buffer_queue_size, logger, disk_queue_file_max_size, max_disk_queue_size)
    @logger = logger
    @mem_max_bytes = mem_max_bytes        # In-memory limit in bytes.
    @mem_queue = []                       # In-memory array for events.
    @mem_current_bytes = 0               # Current used bytes.

    @queue_mutex = Mutex.new
    @queue_not_empty = ConditionVariable.new

    # Initialize the disk-based queue.
    @file_queue = MultiFileQueue.new(
      file_queue_dir,
      pointer_file_path,
      file_buffer_queue_size,
      logger,
      disk_queue_file_max_size,
      max_disk_queue_size
    )
  end

  # -------------------------------------------------------------------------
  # push(item)
  #
  # Pushes an event into memory if there is room (by bytes); otherwise, pushes
  # the event to disk via the MultiFileQueue.
  # -------------------------------------------------------------------------
  def push(item)
    @queue_mutex.synchronize do
      return if item.nil?
      item_size = item.bytesize
      if (@mem_current_bytes + item_size) > @mem_max_bytes
        # Too big for memory, hand off to disk queue.
        @file_queue.push(item)
      else
        @mem_queue << item
        @mem_current_bytes += item_size
      end
      @queue_not_empty.signal
    end
  end

  # -------------------------------------------------------------------------
  # pop
  #
  # Pops an event from memory (if available) or from the disk-based queue.
  # -------------------------------------------------------------------------
  def pop
    @queue_mutex.synchronize do
      while @mem_queue.empty? && @file_queue.size == 0
        @queue_not_empty.wait(@queue_mutex)
      end
      if !@mem_queue.empty?
        item = @mem_queue.shift
        @mem_current_bytes -= item.bytesize
        item
      else
        @file_queue.pop.chomp("\n")
      end
    end
  end

  # -------------------------------------------------------------------------
  # flush_mem_to_disk
  #
  # Returns an array containing the count of in-memory events and the
  # approximate count of disk events.
  # -------------------------------------------------------------------------
  def flush_mem_to_disk
    @queue_mutex.synchronize do
      until @mem_queue.empty?
        item = @mem_queue.shift
        @mem_current_bytes -= item.bytesize
        @file_queue.push(item)
      end
    end

    # Now also force the disk queue to write its in-memory buffer to disk
    @file_queue.flush_queue_force
  end

  # -------------------------------------------------------------------------
  # size
  #
  # Returns an array containing the count of in-memory events and the
  # approximate count of disk events.
  # -------------------------------------------------------------------------
  def size
    [@mem_queue.size, @file_queue.size]
  end

  # -------------------------------------------------------------------------
  # stop
  #
  # Signals the queue to stop
  # -------------------------------------------------------------------------
  def stop
    @queue_mutex.synchronize do
      # wake up anyone waiting on @queue_not_empty
      @queue_not_empty.broadcast
    end
    # also stop the file_queue
    @file_queue.stop
  end
end

# ----------------------------------------------------------------------------- 
# LogStash::Outputs::TcpLoadbalancing
#
# This output plugin uses a combination of memory and disk queues to buffer
# events and then distributes these events to one of multiple hosts 
# (using round-robin selection).
# The plugin handles reconnections, SSL if enabled, and balances writes among hosts.
# -----------------------------------------------------------------------------
class LogStash::Outputs::TcpLoadbalancing < LogStash::Outputs::Base
  include LogStash::PluginMixins::NormalizeConfigSupport
  config_name "tcploadbalancing"

  concurrency :single
  default :codec, "json"

  # Connectivity configuration.
  config :hosts, :validate => :array, :required => true
  config :port, :validate => :number, :required => true
  config :retry_timeout, :validate => :number, :default => 1

  # General configuration.
  config :config_path, :validate => :string, :default => '/usr/share/logstash/pipelines'
  config :lb_method, :validate => ['round_robin'], :default => 'round_robin'

  # Memory queue configuration.
  config :memory_queue_size, :validate => :bytes, :default => 0  # auto-calc if 0
  config :average_item_size, :validate => :bytes, :default => "1Ki"

  # Disk queue configuration.
  config :file_buffer_queue_size, :validate => :bytes, :default => "4Ki" # Buffer size before flush
  config :disk_queue_file_max_size, :validate => :bytes, :default => "10Mi"          # Size for each disk file
  config :max_disk_queue_size, :validate => :bytes, :default => "100Mi"     # Total disk queue limit

  # SSL configuration.
  config :ssl_enabled, :validate => :boolean, :default => false
  config :ssl_certificate, :validate => :path
  config :ssl_key, :validate => :path
  config :ssl_key_passphrase, :validate => :password, :default => nil
  config :ssl_supported_protocols, :validate => ['TLSv1.1', 'TLSv1.2', 'TLSv1.3'], :default => ['TLSv1.3'], :list => true
  config :ssl_cipher_suites, :validate => :string, :list => true

  # -------------------------------------------------------------------------
  # register
  #
  # Sets up the plugin including creating the memory + disk queue,
  # determining the memory queue size (auto-calculated if set to 0),
  # initializing thread pools, and starting SSL or socket reconnection tasks.
  # -------------------------------------------------------------------------
  def register
    LogStash::Util.set_thread_name("[#{pipeline_id}]|output|tcploadbalancing_#{@port}|register")

    queue_dir = "/usr/share/logstash/data/queue/"
    FileUtils.mkdir_p(queue_dir) unless File.directory?(queue_dir)

    jvm_opts = ENV['LS_JAVA_OPTS']
    if jvm_opts.nil?
      raise "#{config_name} needs LS_JAVA_OPTS to be set"
    end
    heap_size = parse_jvm_heap_size(jvm_opts)
    if heap_size.nil?
      raise "#{config_name} needs -Xmx set in LS_JAVA_OPTS"
    end

    # Calculate memory queue size if set to 0.
    tcploadbalancing_count = count_tcploadbalancing(@config_path)
    mem_max_bytes = if @memory_queue_size == 0
      heap_size / tcploadbalancing_count
    else
      @memory_queue_size
    end

    @logger.info("Heap size: #{heap_size} bytes | Memory queue limit: #{mem_max_bytes} bytes")

    # Create a subdirectory for disk queue files.
    base_prefix = "tcploadbalancing_#{@port}"
    plugin_queue_dir = File.join(queue_dir, base_prefix)
    FileUtils.mkdir_p(plugin_queue_dir) unless File.directory?(plugin_queue_dir)

    # Pointer file for the disk queue.
    @pointer_file_path = File.join(queue_dir, "#{base_prefix}.offset")

    # Create the combined memory + disk queue.
    @queue = CustomQueue.new(
      mem_max_bytes,
      plugin_queue_dir,
      @pointer_file_path,
      @file_buffer_queue_size,
      @logger,
      @disk_queue_file_max_size,
      @max_disk_queue_size
    )

    # Initialize thread pool for background tasks.
    @thread_pool = Concurrent::FixedThreadPool.new(1)

    @socket_mutex = Mutex.new
    @socket_created = ConditionVariable.new
    @send_mutex = Mutex.new

    # Initialize SSL if enabled.
    setup_ssl if @ssl_enabled

    # Prepare the TCP socket for each host.
    @sockets = {}
    @hosts.each do |host|
      @sockets[host] = SocketLB.new(nil, CONN_STATUS::CLOSED)
    end

    # For round-robin selection.
    @host_index = 0
    @round_robin_mutex = Mutex.new

    # Begin periodic reconnection attempts for closed sockets.
    @timer_task_sockets = Concurrent::TimerTask.new(execution_interval: @retry_timeout, run_now: true) do
      LogStash::Util.set_thread_name("[#{pipeline_id}]|output|tcploadbalancing_#{@port}|check_sockets")
      check_sockets
    end
    @timer_task_sockets.execute

    # Start the thread that pops events from the queue and sends them.
    @thread_pool.post do
      LogStash::Util.set_thread_name("[#{pipeline_id}]|output|tcploadbalancing_#{@port}|pop_from_queue")
      pop_from_queue
    end

    # Setup the codec to push events into the combined queue.
    push_to_queue
  end

  # -------------------------------------------------------------------------
  # receive
  #
  # Encodes an incoming event using the configured codec.
  # -------------------------------------------------------------------------
  def receive(event)
    @codec.encode(event)
  end

  # ------------------------------------------------------------------------- 
  # close
  #
  # Shuts down background tasks, sends all pending payloads to ensure no data loss,
  # and closes all sockets gracefully.
  # -------------------------------------------------------------------------
  def close
    @stop_flag = true
     
    # Tell the thread pool to stop accepting new tasks and to let existing tasks finish
    @thread_pool.shutdown
    # Optionally wait a few seconds for the `pop_from_queue` thread to exit
    @thread_pool.wait_for_termination(5)

    # This calls CustomQueue#stop, which calls MultiFileQueue#stop
    @logger.debug("Stopping the queue")
    @queue.stop
    
    @logger.debug("Stopping the timer task for sockets")

    @logger.debug("Closing all sockets")
    @timer_task_sockets.shutdown
    @timer_task_sockets.wait_for_termination(2)

    @sockets.each do |_host, socket|
      next if socket.tcp_socket.nil?
      begin
        socket.tcp_socket.close
        socket.state = CONN_STATUS::CLOSED
      rescue => e
        @logger.debug("Error closing socket: #{e.message}")
      end
    end

    @logger.debug("Stopping the thread pool")
    @queue.flush_mem_to_disk
  end

  private

  # -------------------------------------------------------------------------
  # check_socket_status(host)
  #
  # Checks the status of the socket for a given host.
  # Returns false if the socket is closed or remote has closed the connection,
  # otherwise returns true.
  # -------------------------------------------------------------------------
  def check_socket_status(host)
    sock = @sockets[host].tcp_socket
    return false if sock.nil? || @sockets[host].state == CONN_STATUS::CLOSED

    data = sock.read_nonblock(1, exception: false)
    case data
    when nil
      @logger.error("nil read_nonblock => remote closed")
      close_socket(host)
      false
    when :wait_readable
      true
    when ""
      @logger.error("Empty string read_nonblock => remote closed")
      close_socket(host)
      false
    else
      @logger.debug("Received unexpected data from remote: #{data.inspect}, discarding.")
      true
    end
  end

  # -------------------------------------------------------------------------
  # get_host
  #
  # Round-robin selection of a host from the configured list.
  # -------------------------------------------------------------------------
  def get_host
    @round_robin_mutex.synchronize do
      h = @hosts[@host_index]
      @host_index = (@host_index + 1) % @hosts.size
      h
    end
  end

  # Increase or decrease as appropriate
  WRITE_TIMEOUT = 5  # seconds

  # -------------------------------------------------------------------------
  # fetch_batch_from_queue
  #
  # Grabs up to 'max_batch_size' lines from @queue, blocking if needed.
  # Returns an Array of lines (without trailing newlines).
  # -------------------------------------------------------------------------
  def fetch_batch_from_queue(max_batch_size, batch_idle_timeout)
    first_event = @queue.pop  # blocking pop
    return [] if first_event.nil? || first_event.strip.empty?

    batch = [first_event]
    last_event_time = Time.now

    # Try to collect a batch.
    while batch.size < max_batch_size
      mem_size, file_size = @queue.size
      queue_has_data = (mem_size > 0 || file_size > 0)

      if queue_has_data
        evt = @queue.pop
        if evt && !evt.strip.empty?
          batch << evt
          last_event_time = Time.now
        end
      else
        # If we haven't seen new data for 'batch_idle_timeout' seconds, stop.
        break if (Time.now - last_event_time) >= batch_idle_timeout
        sleep(0.05)
      end
    end

    batch
  end

  # -------------------------------------------------------------------------
  # prepare_octet_counted_frames
  #
  # Converts an array of lines (strings) into a single "octet-counted" string.
  # Also returns an array of byte offsets that let us identify partial writes.
  #
  # Example:
  #   lines => ["<14>Test1", "<14>Test2"]
  #   => "7 <14>Test17 <14>Test2"
  #   And an offsets array => [0, length_of_first_frame]
  # -------------------------------------------------------------------------
  def prepare_octet_counted_frames(lines)
    return ["", []] if lines.empty?

    octet_frames = []
    line_offsets = []
    running_offset = 0

    lines.each do |line|
      length = line.bytesize
      frame = "#{length} #{line}"   # RFC 6587: "<length> <data>"
      octet_frames << frame

      line_offsets << running_offset
      running_offset += frame.bytesize
    end

    [octet_frames.join, line_offsets]
  end

  # ------------------------------------------------------------------------- 
  # send_payload
  #
  # 1) Takes an array of lines (strings, no trailing newlines).
  # 2) Builds a single octet-counted string.
  # 3) Non-blocking writes across all hosts in round-robin, partial-write safe.
  # 4) Re-queues unsent lines if all hosts fail.
  # -------------------------------------------------------------------------
  def send_payload(lines)
    return if lines.nil? || lines.empty?
  
    # Convert lines -> single octet-counted string + offsets
    joined_payload, line_offsets = prepare_octet_counted_frames(lines)
  
    total_size = joined_payload.bytesize
    return if total_size == 0
  
    total_written = 0
    fully_sent = false
    sleep_if_all_down = 1
  
    @hosts.size.times do
      host = get_host
      begin
        if check_socket_status(host)
          socket = @sockets[host].tcp_socket
  
          while total_written < total_size
            begin
              written = socket.write_nonblock(
                joined_payload.byteslice(total_written, total_size - total_written)
              )
              total_written += written
  
            rescue IO::WaitWritable
              writable = IO.select(nil, [socket], nil, WRITE_TIMEOUT)
              if writable.nil?
                @logger.warn("Write timeout to #{host}:#{@port}")
                close_socket(host)
                break
              else
                retry
              end
  
            rescue Errno::EPIPE, Errno::ECONNRESET, IOError => e
              @logger.debug("Socket write error to #{host}:#{@port} - #{e.class}: #{e.message}")
              close_socket(host)
              break
  
            rescue => e
              @logger.error("Unknown error writing to socket #{host}: #{e.message}")
              close_socket(host)
              break
            end
          end
  
          # If we wrote the entire payload, success!
          if total_written == total_size
            @socket_mutex.synchronize do
              @sockets[host].messages_sent += lines.size
            end
            @logger.debug("Sent #{lines.size} octet-counted lines to #{host}:#{@port}")
            fully_sent = true
            break
          end
        end
      rescue => e
        @logger.error("Error sending payload to #{host}: #{e.message}")
        close_socket(host)
      end
      break if fully_sent
    end
  
    unless fully_sent
      @logger.error("Failed to send payload to all hosts")
      # # We didn't send all lines; figure out how many were fully sent.
      # # 'line_idx' is the first line whose start offset is strictly greater than total_written.
      # line_idx = line_offsets.bsearch_index { |offset| offset > total_written }
  
      # if line_idx.nil?
      #   # total_written >= the start offset of the last line => partial last line
      #   if total_written < total_size
      #     line_idx = lines.size - 1
      #   else
      #     line_idx = lines.size
      #   end
      # else
      #   # Subtract 1 so line_idx itself is considered unsent if we partially wrote that line
      #   line_idx = [line_idx - 1, 0].max
      # end
  
      # sent_count = line_idx
      # sent_count = 0 if sent_count.negative?  # just in case
  
      # # Log how many lines were *actually* fully sent
      # if sent_count > 0
      #   @socket_mutex.synchronize do
      #     @sockets[host].messages_sent += sent_count
      #   end
      #   @logger.debug("Partially sent batch: successfully sent #{sent_count} lines, " \
      #                "but failed to send the rest.")
      # else
      #   @logger.debug("No lines fully sent in this batch.")
      # end
  
      # # Re-queue everything from line_idx..end
      # if line_idx < lines.size
      #   unsent_lines = lines[line_idx..-1]
      #   unsent_count = unsent_lines.size
  
      #   unsent_lines.each { |l| @queue.push(l) }
  
      #   @logger.debug(
      #     "Re-queued #{unsent_count} lines from index #{line_idx}. " \
      #     "Sleeping #{sleep_if_all_down}s."
      #   )
    end
  end
  

  # ------------------------------------------------------------------------- 
  # pop_from_queue
  #
  # Continuously grabs batches from the queue and calls send_payload
  # with the array of lines (no trailing newlines).
  # -------------------------------------------------------------------------
  def pop_from_queue
    max_batch_size = 200
    batch_idle_timeout = 1  # in seconds

    loop do

      # Ensure at least one socket is open.
      @socket_mutex.synchronize do
        while @sockets.values.all? { |s| s.state == CONN_STATUS::CLOSED } && !@stop_flag
          @socket_created.wait(@socket_mutex, 1)  # Wait 1 second
        end
      end

      break if @stop_flag
      # 1) Fetch a batch from the queue as an array of lines
      batch = fetch_batch_from_queue(max_batch_size, batch_idle_timeout)
      next if batch.empty?

      # 2) Send it (octet counted)
      send_payload(batch)
    end
    logger.debug("Exiting pop_from_queue loop")
  rescue => e
    @logger.error("pop_from_queue loop error: #{e.message}")
    @logger.error(e.backtrace.join("\n"))
  end


  # -------------------------------------------------------------------------
  # close_socket(host)
  #
  # Closes the socket for the given host and marks its state as CLOSED.
  # -------------------------------------------------------------------------
  def close_socket(host)
    @send_mutex.synchronize do
      if @sockets[host].tcp_socket
        @sockets[host].tcp_socket.close rescue nil
      end
      @sockets[host].state = CONN_STATUS::CLOSED
      @sockets[host].tcp_socket = nil
    end
  end

  # -------------------------------------------------------------------------
  # push_to_queue
  #
  # Sets up the codec to push each encoded event into the combined queue.
  # -------------------------------------------------------------------------
  def push_to_queue
    @codec.on_event do |_event, payload|
      @queue.push(payload)
    end
  end

  # -------------------------------------------------------------------------
  # parse_jvm_heap_size
  #
  # Parses LS_JAVA_OPTS to determine the JVM heap size.
  # -------------------------------------------------------------------------
  def parse_jvm_heap_size(jvm_opts)
    if jvm_opts =~ /-Xmx(\d+)([gmk])/i
      size = $1.to_i
      unit = $2.downcase
      case unit
      when 'g' then size * 1024 * 1024 * 1024
      when 'm' then size * 1024 * 1024
      when 'k' then size * 1024
      else size
      end
    else
      nil
    end
  end

  # -------------------------------------------------------------------------
  # check_sockets
  #
  # Periodically attempts to reconnect any closed sockets.
  # -------------------------------------------------------------------------
  def check_sockets
    return if @stop_flag
    @hosts.each do |host|
      @socket_mutex.synchronize do
        next unless @sockets[host].state == CONN_STATUS::CLOSED
        begin
          tcp_socket = TCPSocket.new(host, @port)
          if @ssl_enabled
            require "openssl"
            ssl_socket = OpenSSL::SSL::SSLSocket.new(tcp_socket, @ssl_context)
            ssl_socket.connect
            tcp_socket = ssl_socket
          end
          @sockets[host].tcp_socket = tcp_socket
          @sockets[host].state = CONN_STATUS::OPEN
          @sockets[host].messages_sent = 0
          @socket_created.signal

          @logger.debug("Connected to host #{host}:#{@port}")
        rescue Errno::ECONNREFUSED, SocketError => e
          @logger.debug("Connection refused for #{host}:#{@port} - #{e.message}")
        rescue OpenSSL::SSL::SSLError => ssle
          @logger.debug("SSL Error connecting to #{host}: #{ssle.message}")
        end
      end
    end
  end

  # -------------------------------------------------------------------------
  # pipeline_id
  #
  # Returns the pipeline id, defaulting to 'main' if not set.
  # -------------------------------------------------------------------------
  def pipeline_id
    execution_context.pipeline_id || 'main'
  end

  # -------------------------------------------------------------------------
  # setup_ssl
  #
  # Initializes the OpenSSL context based on configuration.
  # -------------------------------------------------------------------------
  def setup_ssl
    require "openssl"
    @ssl_context = OpenSSL::SSL::SSLContext.new

    if @ssl_certificate
      @ssl_context.cert = OpenSSL::X509::Certificate.new(File.read(@ssl_certificate))
      if @ssl_key
        @ssl_context.key = OpenSSL::PKey::RSA.new(File.read(@ssl_key), @ssl_key_passphrase.value || '')
      end
    end
    @ssl_context.set_params({ verify_mode: OpenSSL::SSL::VERIFY_NONE })

    if ssl_supported_protocols.any?
      disabled_protocols = ['TLSv1.1', 'TLSv1.2', 'TLSv1.3'] - ssl_supported_protocols
      unless OpenSSL::SSL.const_defined?(:OP_NO_TLSv1_3)
        disabled_protocols.delete('TLSv1.3')
      end
      disabled_protocols.map! do |v|
        OpenSSL::SSL.const_get("OP_NO_#{v.sub('.', '_')}")
      rescue NameError
        nil
      end
      disabled_protocols.compact!
      disabled_options = disabled_protocols.reduce(0) { |acc, val| acc | val }
      @ssl_context.options |= disabled_options
    end

    @ssl_context.ciphers = @ssl_cipher_suites if @ssl_cipher_suites&.any?
  end

  # -------------------------------------------------------------------------
  # count_tcploadbalancing
  #
  # Scans the directory for files that reference the tcploadbalancing plugin.
  # Used to count plugin instances for memory queue sizing.
  # -------------------------------------------------------------------------
  def count_tcploadbalancing(dir)
    count = 0
    Find.find(dir) do |path|
      next unless File.file?(path)
      File.open(path, "r") do |file|
        file.each_line do |line|
          next if line.strip.start_with?('#')
          count += 1 if line.include?('tcploadbalancing')
        end
      end
    end
    count
  end
end